#+title: Statistical analysis of Michaelis constant data from online databases
#+author: Teddy Groves and Areti Tsigkinopoulou
#+filetags: :work:
#+startup: overview
#+cite_export: csl ieee.csl
#+LATEX_HEADER: \usepackage{parskip}
#+LATEX_HEADER: \usepackage{color,soul}

#+begin_abstract
We fit a range of Bayesian regression models to analyse data from the BRENDA and
SABIO-RK databases pertaining to Michalis constants or Km parameters. We report
the results and provide tools for reproducing them independently and using them
in a systems biology workflow. We illustrate the intended use of our work with case studies.
#+end_abstract

* Notes                                                            :noexport:
This is a draft of the paper for the [[id:0329A0AE-E75A-4BF5-A8C0-F9669E9EC68F][BRENDA project]] that I am working on with
Areti.

To write comments, use the latex envrionment ~hl~, for example write ~\hl{this is  a comment}~.

* Introduction
The choice of appropriate parameters for biological models has been a recurring
issue for computational biologists in the recent years. As computational models
become increasingly prominent in the current research
[cite:@lopatkinPredictiveBiologyModelling2020;@khoshnawMathematicalModellingCoronavirus2020;@tokicLargescaleKineticMetabolic2020]
and lead the way to biological breakthroughs, the use of high quality parameter
values is becoming essential. In view of these developments, there have been
various efforts to address this issue by employing different
strategies. Although the most popular approach is the estimation of parameters
through optimization methods
[cite:@khodayariGenomescaleEscherichiaColi2016;@saaFormulationConstructionAnalysis2017],
ensemble modelling strategies are rapidly gaining ground in the field of systems
biology
[cite:@tsigkinopoulouRespectfulModelingAddressing2017;@hussainAutomatedParameterEstimation2015;@zhuPredictingSimulationParameters2012;@mourikPredictionUncertaintyAssessment2014]
along with the development of tools to define informative parameter priors
[cite:@saaGeneralFrameworkThermodynamically2015;@mukherjeeNetworkInferenceUsing2008]. At
the same time, machine learning methods have recently been gaining popularity as
a tool for interpreting the large amount of existing biological data
[cite:@bakerMechanisticModelsMachine2018;@xuMachineLearningComplex2019;@zitnikMachineLearningIntegrating2019]
and have also been employed for the prediction of parameter values (mostly Km
values) in biological models
[cite:@yanPredictingKmValues2012;@krollDeepLearningAllows2021;@liDeepLearningBased2021].

By taking into the account the existing issues with model parameterisation and
in an effort to address them, we have previously developed a standardized
protocol for the definition of appropriate parameter lognormal distributions
based on information retrieved from different sources and in accordance with the
modeller’s beliefs about the reliability of each experimental value
[cite:@tsigkinopoulouDefiningInformativePriors2018]. This protocol was a
significant step towards making ensemble modelling more accessible and promoting
interdisciplinary collaborations. However, there were some remaining issues to
be addressed. Firstly, the protocol should take into account the surrounding
parameter landscape (phylogenetically related species, enzyme sub-classes etc.)
in order to avoid having the prior distribution being too narrow or focused only
on one particular area. Furthermore, it should avoid over-reliance on a limited
set of experimental reports.

In order to resolve these problems and additionally make the protocol fully
automated, we devised a novel method for generating appropriate prior
distributions for kinetic parameters. Using previous work by Liebermeister and
Klipp as a starting point [cite:@borgerPredictionEnzymeKinetic2006], we
developed a hierarchical Bayesian regression model in order to perform a
statistical analysis of parameter reports from the BRENDA
[cite:@BRENDASOAPAccess] and SABIO-RK
[cite:@wittigSABIORKDatabaseBiochemical2012] databases. The results of this
analysis can be used in our existing protocol to rationalise parameter weights,
or as part of another protocol.  In order to make it easier to access and review
the information in BRENDA, we also provided an online interface through which
our model’s results can easily be reviewed and extracted.

* State of the art
The problem of modelling kinetic parameter data from online databases has
previously been addressed in several studies.

[cite:@borgerPredictionEnzymeKinetic2006] models logarithmic-scale measurements
$y_{ijk}$ of a Km value for substrate $i$, ec number $j$ and organism $k$ using
the following ANOVA-style regression model:

\begin{equation}
y_{ijk} \sim N(\mu_i + \alpha_ij + \beta_ik, \sigma)
\end{equation}

In this expression the term $\mu$ represents substrate-specific effects, whereas
the terms $\alpha$ and $\beta$ respectively represent substrate-ec number and
substrate-organism interaction effects. $\sigma$ represents the accuracy of the
measurement apparatus.

The authors fit this measurement model to a subset of BRENDA data using maximum
likelihood estimation, obtain out of sample predictions using leave-one-out
cross validation and investigate patterns in the results.

This approach is limited by inability to incorporate prior information about the
plausible values of the main effects or their general trends. In addition, the
analysis produces point estimates: it does not attempt to fully capture the
available information about Kms. This issue is particularly pertinent for
systems biologists who need to construct informative prior distributions for
kinetic models. For these users it is more important to know which possible Km
values are ruled out by the data in BRENDA than it is to know which make that
data most likely. Below we show that both of these issues with the approach in
[19] can be addressed by incorporating a hierarchical Bayesian component.

[cite:@krollDeepLearningAllows2021] and [cite:@liDeepLearningBased2021] predict
kinetic parameters from BRENDA and other sources by taking into account protein
and substrate structure information using neural networks. Exploiting this
information allows for improved out of sample predictive performance compared to
models that do not use it.

However, this approach does not avoid the issues with lack of prior information
and inaptness for downstream prior modelling that we highlight above. In
addition, the need for information about protein structure limits the amount of
kinetic parameters for which predictions can be obtained. Again taking the point
of view of a systems biologist attempting to construct informative prior
distributions, this is a severe problem as coverage is at least as important a
consideration for this application as precision.

* Method
** Data fetching
We fetched data from the SABIO-RK [cite:@wittigSABIORKDatabaseBiochemical2012]
and BRENDA [cite:@changBRENDAELIXIRCore2021] databases using their publicly
available APIs. We also fetched a list of all EC numbers from the Expasy
database [cite:@duvaudExpasySwissBioinformatics2021]. In the project repository,
see the script ~fetch_data.py~ and the library file ~fetching.py~ for code used
to fetch data and the directory ~data/raw/~ for the results.

In total we fetched \hl{EXACT NUMBER HERE} raw BRENDA Km reports, \hl{EXACT NUMBER HERE} SABIO-RK Km reports and \hl{EXACT NUMBER HERE} Expasy EC numbers.

** Data processing
We made several significant data processing choices. See the library file
~data_preparation.py~ for code used to implement these choices.

*** Filtering
For each dataset and kinetic parameter, we removed all reports which failed to
satisfy any of the following conditions:

- The kinetic parameter value must be a number
- The literature reference must not be missing
- The substrate must be catalysed naturally by the enzyme
- The enzyme must be from a wild organism
- The temperature, if recorded, must be between 10 and 45 degrees C 
- The pH, if recorded, must be between 5 and 9
- The organism must be have data from at least 50 separate study/biology
  (i.e. organism:substrate:ec4 for BRENDA or organism:substrate:enzyme for
  SABIO-RK) combinations that satisfy all the other conditions.

*** Grouping
Instead of modelling reports directly, we chose to group together reports with
the same biology and study, treating the median log-scale km as a single
observation. We took this decision because of the presence in both datasets of
different kinds of study. In some cases - presumably when the aim of a study was
to discover the sensitivity of a kinetic parameter to changes in conditions -
many reports with the same enzyme, organism, substrate and study are available,
with a range of different kinetic parameter values and different experimental
conditions recorded in the ~commentary~ field. In other cases a study will
report only a single value for one kinetic parameter.

Due to this discrepancy it seemed wrong to treat reports from better populated
studies as equivalent to reports from more concise studies. While taking the
median for a given study/biology combination before modelling destroys
information, we judged that it would lead to more realistic results than
treating each report as an observation, especially since we chose not to
attempt to model the effects of experimental conditions.

** Statistical model
We used a Bayesian hierarchical regression model to describe all data. Since Km
parameters are constrained to be positive, we modelled them on natural
logarithmic scale, using the same approach taken in
[cite:@borgerPredictionEnzymeKinetic2006], but replacing the organism:substrate
interaction with enzyme:substrate, where this information is available.

Our model includes a global mean parameter $\mu$, hierarchical
substrate-specific intercept parameters $a^{sub}$ and hierarchical intercept
parameters $a^{enz:sub}$, $a^{ec4:sub}$ and specific to interactions of
substrate and enzyme and ec4 number respectively. In addition we used a
student-T measurement model with latent standard deviation and degrees of
freedom $\sigma$ and $\nu$.

We chose assigned semi-informative prior distributions based on the
pre-experimental information. In particular, the prior for the measurement
distribution degrees of freedom parameter $\nu$ follows the recommendation in
[cite:@juarezModelBasedClusteringNonGaussian2010] and is truncated below at 2,
reflecting our view that the measurement distribution should not be excessively
heavy-tailed. The prior for the enzyme:substrate interaction effect is also put
close to zero, reflecting our view that, though non-negligible, this effect is
likely to be smaller than the others.

The full model specification in tilde notation is shown below. In these lines
the symbol $\tilde$ should be understood as meaning "has the
distribution". $ST$, $N$, $LN$ and $\Gamma$ represent the student-t, normal,
log-normal and gamma distributions respectively. Square brackets represent
truncated probability distributions. For example $X \tilde N(0,1)[0, \infty]$
means that the variable $X$ has a normal distribution truncated below at zero,
also known as a "half-normal" distribution.

\begin{align*}
\ln{y} &\sim ST(\nu, \hat{\ln{y}}, \sigma) \\
\hat{\ln{y}} &= \begin{cases}
\textbf{enz available : } \mu + a^{sub} + a^{ec4:sub} + a^{enz:sub} \\
\textbf{otherwise : } \mu + a^{sub} + a^{ec4:sub} \\
\end{cases} \\
a^{sub} &\sim N(0, \tau^{sub}) \\
a^{ec4:sub} &\sim N(0, \tau^{ec4:sub}) \\
a^{enz:sub} &\sim N(0, \tau^{enz:sub}) \\ 
\mu &\sim N(-2, 2) \\
\nu &\sim \Gamma(2, 0.1)[2, \infty] \\
\sigma &\sim LN(0, 0.2) \\
\tau^{sub} &\sim LN(0, 0.3) \\
\tau^{ec4:sub} &\sim LN(0, 0.2) \\
\tau^{enz:sub} &\sim LN(-2, 0.2) \\
\end{align*}

Prior distributions for parameters with log-normal priors are shown graphically
in figure [[fig:sd_priors]].

#+CAPTION: KDE plots of prior distributions for log-normal parameters
#+ATTR_ORG: :width 600
#+NAME: fig:sd_priors
[[./results/plots/sd_priors_enz.png]]

Our model can be expressed as the following program in the probabilistic programming language Stan program [cite:@carpenterStanProbabilisticProgramming2017]:

#+include: "src/stan/enz.stan" src stan

For the BRENDA data, we used the same model but removed the enzyme-substrate
interaction parameters as BRENDA does not provide enzyme-specific information.

** Model validation
We used a number of standard methods to test our models' validity, aiming to
follow the method described in [cite:@gelmanBayesianWorkflow2020]. To verify the
computation we used standard MCMC convergence metrics and fit our model to fake
data. To assess model specification we performed graphical prior and posterior
predictive checks and both approximate and exact cross validation, using some
simple test models for comparison.

*** Computation
For each MCMC run we used arviz [cite:@kumarArviZUnifiedLibrary2019] to
calculate the improved \hat{R} statistic following the method in
[cite:@vehtariRankNormalizationFoldingLocalization2021] and to check for
divergent transitions. If the \hat{R} statistic was sufficiently close to 1
(i.e. +/- 0.03) and there were no post-warmup divergent transitions we judged
that the computation was likely to have been successful in the sense that the
draws can be treated as samples from the target distribution.

To further validate the computation we also fit the model to fake data generated
using the model assumptions and a plausible configuration of parameters. We used
graphical posterior predictive checks to assess whether the true parameters were
approximately recovered in the posterior distribution. See supplementary
material for graphical posterior predictive checks with fake data.

*** Comparison models
Ideally we would compare the results of our models with previously published
attempts to model the same data. However this was not practical in our case for
two reasons. First, other results are typically derived from different raw
datasets, and rely on different data filtering and summarising decisions. In
particular, we have not previously seen the problem of whether and how to
aggregate results from the same study addressed in detail. Second, the other
results typically assessed model specification by comparing observed data to
point predictions generated by their models. Applying this procedure to our
models would give an incomplete picture, since a single point cannot adequately
summarise the full posterior predictive distribution. In addition, it would not
align with the priorities of our intended users, namely systems biologists
constructing prior distributions for the purpose of ensemble modelling, who we
imagine care more about the extremes of the distributions than central
estimates.

Due to this lack, we constructed two simple models purely for comparison with
our main model. The first model, which we called the "really simple" model,
removes all latent random variables from the main model's predictor, except for
the global mean parameter \mu. The second model, which we called the "simple"
model, includes a single vector of hierarchical parameters at the finest
available granularity, i.e. organism:substrate:ec4 for BRENDA data and
organism:substrate:enzyme for SABIO-RK data. See supplemental information for
full model specifications in the form of tilde notation and Stan programs.

Our thinking was that the really simple model and simple models represent
relative extremes of underfitting and overfitting. A well-specified model should
be able to make better predictions than both, unless the data are very
surprisingly uninformative.

In addition to these comparison models, we also compared our final model with a
Bayesian version of the model in [cite:@borgerPredictionEnzymeKinetic2006],
which we obtained simply by removing the parameters $a^{enz:sub}$ and
$\tau^{enz:sub}$ from our final model. We included this extra test in order to
verify whether including enzyme-specific parameters was worthwhile.

*** Graphical posterior predictive checks
Figure [[fig:ppc_km_sabio_enz]] below shows our main model's marginal posterior
predictive 1%-99% interval for each observation, alongside the observed
value. Ideally exactly 98% of observations should be covered, and whether or not
an observation is covered should not be systematically predictable.

#+CAPTION: Marginal posterior predictive distributions for main model fit to SABIO Km data
#+NAME: fig:ppc_km_sabio_enz
[[./results/plots/ppc_sabio-km-enz.png]]

Figures [[fig:ppc_km_sabio_simple]] and [[fig:ppc_km_sabio_really_simple]] show
graphical posterior predictive checks for the really simple and simple models
fit to the same dataset.

#+CAPTION: Marginal posterior predictive distributions for simple model fit to SABIO Km data
#+NAME: fig:ppc_km_sabio_simple
[[./results/plots/ppc_sabio-km-simple.png]]

#+CAPTION: Marginal posterior predictive distributions for really simple model fit to SABIO Km data
#+NAME: fig:ppc_km_sabio_really_simple
[[./results/plots/ppc_sabio-km-really-simple.png]]

From the continuous line of black dots in figure [[fig:ppc_km_sabio_simple]] we can
see that the posterior predictive intervals have approximately the same order as
the observed data, suggesting that the model is likely overfitting, or at least
not using any structural information from the data.

In contrast figure [[fig:ppc_km_sabio_really_simple]] shows that the really simple
model is clearly underfitting the observed data - there is no relationship
between the order of the black dots and the blue lines.

Overall these graphical posterior predictive checks made us confident that our model was achieving a qualitatively satisfactory balance between overfitting and underfitting. However, from graphical posterior predictive checks it was not easy to distinguish our final model from the BLK model. Figure [[fig:ppc_km_sabio_blk]]
shows the posterior predictive check graph for this model: there is no clear sign of overfitting or under-fitting.

#+CAPTION: Marginal posterior predictive distributions for BLK model fit to SABIO Km data
#+NAME: fig:ppc_km_sabio_blk
[[./results/plots/ppc_sabio-km-blk.png]]

In order to evaluate the two models in more detail, we compared their out-of-sample predictive performance using a cross-validation approach.

*** Cross validati
For a quantitative compliment to the graphical posterior predictive checks, we
used cross validation to assess our models' out-of-sample predictive
performance. For exploratory comparison we calculated each model's approximate
leave-one-out expected log predictive density using the method set out in
[cite:@vehtariPracticalBayesianModel2017] and implemented in
[cite:@kumarArviZUnifiedLibrary2019]. Since the diagnostics suggested that the
approximate leave-one-out algorithm was likely to be unreliable, we also performed
exact five-fold cross validation.

The results were as follows:

#+CAPTION: Average out-of-sample log likelihoods for the SABIO-RK dataset
|-------+--------------------------------------|
| Model | Average out-of-sample log likelihood |
|-------+--------------------------------------|
| BLK   |                               -2.744 |
| final |                               -2.693 |
|-------+--------------------------------------|

The final model made slightly better out-of-sample predictions. The distribution of log likelihoods by posterior sample can be seen in figure [[fig:cv]] below.

#+CAPTION: Histograms of out-of-sample log likelihoods
#+LABEL: fig:cv
#+ATTR_ORG: :width 600
[[./results/plots/cv.png]]

This leave-one-fold-out cross validation procedure is somewhat flawed because,
unlike with leave-one-observation-out cross validation or in a real-life
prediction scenario, the test dataset contains many unseen parameter
combinations. However, this problem is likely to favour the BLK model over the
final model, as there are tend to be more unseen enzyme/substrate combinations
than unseen organism/substrate combinations. This test therefore satisfied us
that the final model was an improvement on the BLK model for modelling the
SABIO-RK dataset.

** Web app
In order to make our results accessible to the systems biology community, we wrote a web app that presents them in an easy to use and actionable format. This can be found at \hl{URL}.

The user can choose a dataset, organism, substrate and enzyme and is then
presented with a KDE summary of the corresponding marginal posterior
distribution and some summary statistics describing it, which can be used to
inform a choice of priors.

For each biological category there is an option to choose "Unknown" - in this
case the relevant marginal posterior is calculated dynamically.

The app also provides links from which tables summarising all the marginal
posteriors can be downloaded.

The app was written using Streamlit [cite:@StreamlitFastestWay].

* Results
** Marginal distributions of interesting parameters
We begin by looking at the marginal distributions of the model's standard
deviation parameters. Comparing these indicates which effects varied the
most. The comparison in figure [[fig:taus]] shows that the most important
hierarchical parameter is the substrate effect, followed by the ec4/substrate
interaction, the enzyme/substrate interaction and finally the organism/substrate
interaction. All of the distributions tend to be greater than that of the
estimated measurement error ~sigma~, suggesting that our model is able to
distinguish all the effects from measurement noise. The substrate and
ec4:substrate taus tend to be the greatest, suggesting that these are the most important predictors.

#+CAPTION: Comparison of hierarchical standard deviation parameters
#+LABEL: fig:taus
#+ATTR_ORG: :width 600
[[./results/plots/sd_posteriors_enz.png]]

Figure [[fig:brenda_sabio_comparison]] compares the marginal distributions of km
parameters from our best SABIO-RK and BRENDA models. The marginal posterior
distributions in the BRENDA model tend to be narrower, reflecting the larger
input dataset.

#+CAPTION: Comparison of measured and modelled Km parameter distributions in BRENDA and SABIO-RK
#+LABEL: fig:brenda_sabio_comparison
#+ATTR_ORG: :width 600
[[./results/plots/log_km_comparison.png]]

\hl{What else to put here? Distributions of $\ln$ kms?}
** Comparison of estimated Kms with physiological metabolite concentrations
Several sources suggest that the Km parameter for a particular organism,
substrate and enzyme should have the same order of magnitude as the typical
physiological concentration of that substrate. \hl{ANOTHER REFERENCE HERE?} For example
[cite:@borgerPredictionEnzymeKinetic2006] write

#+begin_quote
...we may hypothesize that KM values are adjusted to the order of magnitude of
the substrate concentration. If this is the case and if a metabolite exhibits a
particularly high concentration in a certain organism, then all corresponding KM
values should also tend to be increased.
#+end_quote

In order to test whether this is the case, we obtained data about some
physiological metabolite concentrations from SABIO-RK and compared these with
the corresponding Km measurements and marginal posterior distributions from our
model. Figure [[fig:hmdb]] shows the results.

#+CAPTION: Comparison of model predictions with physiological metabolite concentrations from HMDB
#+ATTR_ORG: :width 600
#+LABEL: fig:hmdb
[[./results/plots/concentration_comparison.png]]

The graph on the left shows that our data somewhat support the first part of the
hypothesis: the physiological concentrations are distributed similarly to the
posterior Km samples. The graph on the right suggests that $\ln$ scale Kms tend to be 1-2 $\ln$ mM lower than their corresponding physiological concentration reports, give or take about 5 $\ln$ mM. In other words, there is only a fairly weak relationship between a particular Km and its corresonding physiological concentration.

This is reflected in our model's marginal posterior distribution for the
parameter $\tau^{org:sub}$, which concentrates closer to zero than any of the
other hierarchical standard deviation parameters \hl{CHECK THIS IS TRUE}, as can be
seen in figure [[fig:taus]]

** NADH vs NADPH

Figure [[fig:nadh]] shows histograms of measured Km parameters from the SABIO-RK
dataset for NADH and NADPH binding enzymes alongside histograms of our final
model's posterior samples for these parameters. We can see that the measurements
for NADPH tended to be higher, and this difference is also present in the posterior
distributions.

#+CAPTION: Comparison of modelled and measured Km parameters for NADH and NADPH
#+LABEL: fig:nadh
#+ATTR_ORG: :width 600
[[./results/plots/nadh.png]]

** Cofactor vs substrate
We tested whether there tended to be a systematic difference between the km of
an enzyme's main substrate and that of a cofactor. 

Figure [[fig:cofactor_effects]] shows the 1%-99% posterior intervals for substrate
effects of cofactors superimposed on those of all substrates. There does not
seem to be a particular pattern

#+CAPTION: Comparison of modelled and measured Km parameters for NADH and NADPH
#+LABEL: fig:cofactor_effects
#+ATTR_ORG: :width 600
[[./results/plots/cofactor_effects.png]]

Figure [[fig:cofactor_comparison]] shows, for each reaction for which both a
cofactor and substrates were measured in the SABIO-RK dataset, the posterior mean of the cofactor and that of the substrate. Again there is no particular pattern

#+CAPTION: Comparison of modelled and measured Km parameters for NADH and NADPH
#+LABEL: fig:cofactor_comparison
#+ATTR_ORG: :width 600
[[./results/plots/cofactor_substrate_comparison.png]]

** Which results to use?

Our results make it possible to compare the SABIO-RK and BRENDA datasets,
allowing practitioners to make more informed modelling decisions.

The SABIO-RK dataset provides more specific information than the BRENDA
datase. UNIPROT ids are available for many Km measurements, whereas the most
specific enzyme-level information in the BRENDA dataset is the EC4 number. In
addition, in contrast to BRENDA, the SABIO-RK dataset provides information about
substrate, literature reference, enzyme, temperature, pH and strain type in
interoperable formats and without the need for potentially error-prone string
operations.

As a result of this extra information, the model fit to the SABIO-RK data is
more specific, allowing modellers to obtain priors that reflect measurements of
the exact enzyme they wish to model while still appropriately taking into
account measurements of related enyzmes. In addition, the SABIO model excludes
some potential sources of bias, such as reports from non-wildtype strains and
atypical conditions that are not accounted for in the BRENDA model.

The SABIO dataset is worse than the BRENDA dataset in one respect, however: at
the time of writing there are far more reports in the BRENDA dataset. The model
fit to the BRENDA dataset is therefore able to take into account more
information than is available to the SABIO-RK model.

There are therefore potentially conflicting factors to take into account when
deciding which dataset to use in a given case. A further complication is that,
due to the lack of interoperable identifiers in the BRENDA dataset, it is
difficult to directly compare the which yields better predctions.

In our judgment the best approach given these circumstances is to favour the
SABIO-RK results in cases where they provides a lot of information for a given
parameter or when the information in BRENDA is too coarse or likely to be
biased. In other cases the BRENDA results should be preferred.

Our results make it easier to assess which of these two scenarios apply. The
information available for a given km is captured by the marginal posterior
distribution for that parameter - if the marginal posterior distribution for a
parameter in the SABIO model is very narrow, this means that the SABIO dataset
contains a lot of information about that parameter. In order to assess the
liklihood of the BRENDA dataset being too coarse for a parameter whose enzyme
shares an EC number, the marginal prior distributions of the common parameters
can be compared. If they are very different, it is likely better to use the more
specific SABIO results. Finally, in order to assess the likelihood of bias in
the BRENDA data, our webapp provides links to the source documents for any
parameter measurement.

* TODO Case studies
* TODO Discussion
* References

#+PRINT_BIBLIOGRAPHY:
