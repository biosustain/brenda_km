#+TITLE: Analysing some results
#+PROPERTY: header-args:jupyter-python :async yes :exports both
#+STARTUP: overview

This notebook analyses the results of a model of some BRENDA km data. To generate the results yourself, first make sure you have installed all the dependencies, then run these python scripts:

#+begin_example sh
python prepare_data.py
python generate_fake_data.py
python sample.py
#+end_example

* Notes                                                            :noexport:

#+begin_src emacs-lisp
(setq-local org-image-actual-width '(500))
#+end_src

#+RESULTS:
| 500 |


* Preliminaries

First make sure you have already run some models. If you have, there should be
some subfolders in the folder ~results/runs~. We check this with the following
shell command:

#+begin_src sh :results drawer :display raw
ls results/runs
#+end_src

#+RESULTS:
:results:
blk
readme.md
really_simple
simple
:end:

There are some folders there so we are good!

* Imports

Now we start writing python code for analysis. First some imports

#+begin_src jupyter-python :session py :exports both :results none :async yes 
  import json
  import os
  import warnings

  import arviz as az
  import numpy as np
  import pandas as pd
  from matplotlib import pyplot as plt
  from matplotlib.axes._axes import Axes as MplAxes
#+end_src

* Loading the input data

Load a csv table ~m~ of prepared measurements.

#+begin_src jupyter-python :session py :exports both :results none :async yes :display plain
  DATA_DIR = os.path.join("data", "prepared", "tenfold")
  m = pd.read_csv(os.path.join(DATA_DIR, "input_df.csv"))
#+end_src

The table's rows report the mean reported value for each study/km combination
we retrieved from the BRENDA database, for the organisms Escherichia coli, Homo
Sapiens and Saccharomyces cerevisiae.

Here is what the first row looks like:

#+begin_src jupyter-python :session py :exports both :results raw drawer :async yes :display plain
  m.iloc[0]
#+end_src
#+RESULTS:
:results:
#+begin_example
  Unnamed: 0                                        0
  ec4                                       1.1.1.100
  organism                           Escherichia coli
  substrate                                     NADPH
  ec_sub                              1.1.1.100|NADPH
  org_sub                      Escherichia coli|NADPH
  literature                                 [704337]
  biology            1.1.1.100|Escherichia coli|NADPH
  y                                          -4.60517
  n                                                 1
  sd                                              NaN
  ec4_stan                                          1
  organism_stan                                     1
  substrate_stan                                    1
  ec_sub_stan                                       1
  org_sub_stan                                      1
  literature_stan                                   1
  biology_stan                                      1
  Name: 0, dtype: object
#+end_example
:end:

Here are the y values in order:

#+begin_src jupyter-python :session py :exports both :results drawer :async yes
  def plot_data(ax: MplAxes, m: pd.DataFrame, obs_col="y"):
      x = np.arange(len(m))
      y = m.sort_values(obs_col)[obs_col]
      ax.scatter(x, y, color="black", s=5, label="Observed")
      ax.get_xaxis().set_ticks([])
      ax.set(xlabel="Ec/substrate/organism in a study",
             ylabel="km ($\ln$ scale)")
      leg = ax.legend(frameon=False)
      return ax

  f, ax = plt.subplots()
  ax = plot_data(ax, m)
  f.savefig(
      os.path.join("results", "plots", "input_data.svg"),
      bbox_inches="tight"
  )
#+end_src

#+RESULTS:
:results:
:RESULTS:
#+attr_org: :width 394
[[file:./.ob-jupyter/7cf6715bf9152509a368ec51b6b0561c30df07f3.png]]
:END:
:end:
:end:
:end:

I thought it might be interesting to see how the input data varies as a function of the organism. The plot below plots kms in our dataset that have measurements for all three organisms:

#+begin_src jupyter-python :session py :exports both :results raw drawer :async yes

  def plot_organism_heterogeneity(m: pd.DataFrame):
      groupcols = ["ec4", "substrate", "organism"]
      t = (
          m.groupby(groupcols)["y"].mean()
          .unstack("organism")
          .dropna(how="any")
          .sort_values("Escherichia coli")
      )
      f, ax = plt.subplots(figsize=[8,4])
      for col in t.columns:
          ax.scatter(range(len(t)), t[col].values, label=col)
      ax.get_xaxis().set_ticks([])
      ax.legend(frameon=False)
      ax.set(
          title="Between organism variation",
          xlabel="Ec/substrate/organism in a study",
          ylabel="Average km ($\ln$ scale)"
      )
      return f, ax
  f, ax = plot_organism_heterogeneity(m)

  f.savefig(
      os.path.join("results", "plots", "heterogeneity.svg"),
      bbox_inches="tight"
  )
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/d6f0a13737b739e435b21d0ed422b75b8be6833e.png]]
:results:
:end:
:end:

* Checking model results
Now we choose some files to analyse and load them into arviz [[https://arviz-devs.github.io/arviz/api/inference_data.html][InferenceData]]
objects.

#+begin_src jupyter-python :session py :exports both :results drawer :display plain :async yes
  RESULTS_DIR = os.path.join("results", "runs", "blk")
  idata = az.from_netcdf(os.path.join(RESULTS_DIR, "posterior", "idata.nc"))
  prior = az.from_netcdf(os.path.join(RESULTS_DIR, "prior", "idata.nc"))
  fake = az.from_netcdf(os.path.join(RESULTS_DIR, "fake", "idata.nc"))
  idata.add_groups({
     "prior": prior.posterior,
     "fake": fake.posterior,
     "observed_data_fake": fake.observed_data,
     "sample_stats_prior": prior.sample_stats
  })
  idata
#+end_src

#+RESULTS:
:results:
: /Users/tedgro/.pyenv/versions/3.8.6/lib/python3.8/site-packages/arviz/data/inference_data.py:1317: UserWarning: The group fake is not defined in the InferenceData scheme
:   warnings.warn(
: /Users/tedgro/.pyenv/versions/3.8.6/lib/python3.8/site-packages/arviz/data/inference_data.py:1317: UserWarning: The group observed_data_fake is not defined in the InferenceData scheme
:   warnings.warn(
: Inference data with groups:
: 	> posterior
: 	> log_likelihood
: 	> sample_stats
: 	> observed_data
: 	> prior
: 	> fake
: 	> observed_data_fake
: 	> sample_stats_prior
:end:

#+begin_src jupyter-python :session py :exports both :results drawer :display plain :async yes
  ppq_prior, ppq_posterior, ppq_fake = (
      idata.get(g)["yrep"]
      .quantile([0.025, 0.5, 0.975], dim=["chain", "draw"])
      .to_series()
      .unstack("quantile")
      .add_prefix(f"{g}_")
      .rename_axis("", axis=1)
      for g in ("prior", "posterior", "fake")
  )
  ppq = ppq_prior.join(ppq_posterior).join(ppq_fake)
  ppq["obs"] = idata.observed_data["y_test"].values
  ppq["obs_fake"] = idata.observed_data_fake["y_test"].values
  ppq = ppq.sort_values("obs")
  ppq
#+end_src

#+RESULTS:
:results:
#+begin_example
              prior_0.025  prior_0.5  prior_0.975  posterior_0.025  \
  yrep_dim_0                                                         
  1165          -7.847306  -2.001490     4.176406       -13.393017   
  135           -7.782696  -2.018565     3.902972        -6.582813   
  4031          -7.787102  -1.982730     3.821657       -16.086117   
  4353          -8.103219  -2.064030     4.028205       -11.915835   
  3028          -7.791763  -1.966985     3.845696       -12.640320   
  ...                 ...        ...          ...              ...   
  5262          -8.062909  -2.015935     4.032168        -6.012040   
  5691          -8.200756  -2.026690     4.004418        -5.219667   
  299           -7.903204  -1.958700     4.182943        -3.605605   
  5648          -7.958700  -2.016490     4.143261         1.439340   
  5143          -7.899296  -1.979665     3.928554        -4.157994   

              posterior_0.5  posterior_0.975  fake_0.025  fake_0.5  fake_0.975  \
  yrep_dim_0                                                                     
  1165            -9.893235        -6.424276   -6.279775 -3.906175   -1.592293   
  135             -3.463750        -0.101759   -5.991642 -3.783785   -1.630591   
  4031            -7.743000         3.197087   -4.725760 -1.926205    0.647097   
  4353            -8.289100        -3.854086   -2.135240  0.343630    2.713712   
  3028            -5.245645         1.228611   -6.759330 -4.060620   -0.884686   
  ...                   ...              ...         ...       ...         ...   
  5262             1.625280         7.477105   -5.246686 -2.507475    0.309686   
  5691            -1.842900         1.685722   -3.634911 -1.439110    0.726355   
  299              0.770304         4.699433   -5.198901 -2.901570   -0.589103   
  5648             4.711475         8.232254   -6.388566 -4.136915   -1.837142   
  5143            -0.656408         2.828056   -5.626431 -3.240725   -0.802943   

                    obs  obs_fake  
  yrep_dim_0                       
  1165       -17.777735 -4.224110  
  135        -16.436196 -4.141880  
  4031       -15.855731  0.043783  
  4353       -15.476242  0.265162  
  3028       -14.865333 -4.528010  
  ...               ...       ...  
  5262         6.040255 -2.620180  
  5691         6.492240 -1.913510  
  299          6.522093 -2.565750  
  5648         6.620919 -4.305300  
  5143         7.176002 -3.377470  

  [6389 rows x 11 columns]
#+end_example
:end:

This code plots the predictive distributions for the prior and posterior against the real observations:

#+begin_src jupyter-python :session py :exports both :results raw drawer :async yes
  f, axes = plt.subplots(1, 2, sharey=True, figsize=[15, 5])
  axes = axes.ravel()
  d = ppq.sort_values("obs")
  x = np.arange(len(d))
  for g, ax in zip(["prior", "posterior"], axes):
      ax = plot_data(ax, d, obs_col="obs")
      ax.vlines(
          x, d[f"{g}_0.025"], d[f"{g}_0.975"],
          color="tab:blue", zorder=0, label="2.5%-97.5% interval"
      )
      leg = ax.legend(frameon=False)
      ax.set_title(g.capitalize())
  
  f.savefig(
      os.path.join("results", "plots", "ppc.svg"),
      bbox_inches="tight"
  )
#+end_src

#+RESULTS:
:results:
[[file:./.ob-jupyter/6cf630bb77131d709a5c9663c691ad1b4b09ecd4.png]]
:end:
:end:


This code does the same for the fake data:

#+begin_src jupyter-python :session py :exports both :results raw drawer :async yes
  f, ax = plt.subplots(figsize=[8,4])
  d = ppq.sort_values("obs_fake")
  x = np.arange(len(d))
  ax = plot_data(ax, d, obs_col="obs_fake")
  ax.vlines(
      x, d["fake_0.025"], d["fake_0.975"],
      color="tab:blue", zorder=0, label="2.5%-97.5% interval"
  )
  leg = ax.legend(frameon=False)
  ax.set_title("Fake data posterior predictive distribution")
  f.savefig(
      os.path.join("results", "plots", "ppc_fake.svg"),
      bbox_inches="tight"
  )
  
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/2d3d56b14ac506c98fd9d5e4c41c7602d1dd794c.png]]
:results:
:end:
:results:
:end:
:end:

* Cross validation

We can try to evaluate the model's predictive performance using approximate
leave-one-out cross validation.

#+begin_src jupyter-python :session py :exports both :results raw drawer :async yes :display plain
  az.loo(idata, pointwise=True)
#+end_src

#+RESULTS:
:results:
: /Users/tedgro/.pyenv/versions/3.8.6/lib/python3.8/site-packages/arviz/stats/stats.py:655: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.
:   warnings.warn(
#+begin_example
  Computed from 3200 by 6389 log-likelihood matrix

           Estimate       SE
  elpd_loo -12549.28    86.78
  p_loo     2681.82        -

  There has been a warning during the calculation. Please check the results.
  ------

  Pareto k diagnostic values:
                           Count   Pct.
  (-Inf, 0.5]   (good)     5914   92.6%
   (0.5, 0.7]   (ok)        407    6.4%
     (0.7, 1]   (bad)        65    1.0%
     (1, Inf)   (very bad)    3    0.0%
#+end_example
:end:

Unfortunately there are quite a few observations with high pareto k values, and arviz raises a warning.

To address this issue we also ran exact ten-fold cross validation.

The code below checks the results of these finds the posterior mean for
out-of-sample log likelihood for each split in our model and compares the
results with two alternative models:

#+begin_src jupyter-python :session py :exports both :results raw drawer :async yes :display plain
  run_dir = os.path.join("results", "runs")
  runs = [
      os.path.join(run_dir, s) for s in os.listdir(run_dir)
      if os.path.isdir(os.path.join(run_dir, s))
  ]
  for run in runs:
      run_ll = 0
      splits_dir = os.path.join(run, "splits")
      print(f"calculating out of sample log likelihoods for model {run}...")
      for split in os.listdir(splits_dir):
          idata = az.from_netcdf(os.path.join(splits_dir, split, "idata.nc"))
          split_ll = (
              idata.get("log_likelihood")["llik"]
              .mean(dim=["chain", "draw"])
              .values.sum()
          )
          print(f"\t{split}: {str(split_ll)}")
          run_ll += split_ll
      print(f"\ttotal out of sample log likelihood: {run_ll}")
#+end_src

#+RESULTS:
:results:
: calculating out of sample log likelihoods for model results/runs/blk...
: 	split_4: -1477.2729990849998
: 	split_3: -1481.00882687375
: 	split_2: -1441.2448831225
: 	split_5: -1477.1304594875
: 	split_0: -1467.7910366725
: 	split_7: -1480.53716923375
: 	split_9: -1504.9884546787503
: 	split_8: -1502.4060516825002
: 	split_6: -1483.4119273775
: 	split_1: -1515.77739092
: 	total out of sample log likelihood: -14831.569199133752
: calculating out of sample log likelihoods for model results/runs/simple...
: 	split_4: -2245.9115185025003
: 	split_3: -2182.5809449999997
: 	split_2: -2262.4983365774997
: 	split_5: -2150.721065105
: 	split_0: -2351.9525779625
: 	split_7: -2288.8911877974997
: 	split_9: -2194.1904108999997
: 	split_8: -2348.7044462475
: 	split_6: -2389.71007908
: 	split_1: -2263.04032354
: 	total out of sample log likelihood: -22678.2008907125
: calculating out of sample log likelihoods for model results/runs/really_simple...
: 	split_4: -1552.20558885
: 	split_3: -1505.90971685
: 	split_2: -1551.5482681499998
: 	split_5: -1470.82288385
: 	split_0: -1639.23052325
: 	split_7: -1557.061260825
: 	split_9: -1516.690492875
: 	split_8: -1651.7273156750002
: 	split_6: -1680.8369810999998
: 	split_1: -1575.714097825
: 	total out of sample log likelihood: -15701.747129249998
:end:
:end:
:end:
:end:
