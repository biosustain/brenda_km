#+TITLE: Analysing some results
#+PROPERTY: header-args:jupyter-python :async yes :exports both
#+STARTUP: overview

This notebook analyses the results of a model of some BRENDA km data. To generate the results yourself, first make sure you have installed all the dependencies, then run these python scripts:

#+begin_example sh
python prepare_data.py
python generate_fake_data.py
python sample.py
#+end_example

* Notes                                                            :noexport:

#+begin_src emacs-lisp
(setq-local org-image-actual-width '(500))
#+end_src

#+RESULTS:
| 500 |


* Preliminaries

First make sure you have already run some models. If you have, there should be
some subfolders in the folder ~results/runs~. We check this with the following
shell command:

#+begin_src sh :results drawer :display raw
ls results/runs
#+end_src

#+RESULTS:
:results:
blk
readme.md
really_simple
simple
:end:

There are some folders there so we are good!

* Imports

Now we start writing python code for analysis. First some imports

#+begin_src jupyter-python :session py :exports both :results none :async yes 
  import json
  import os
  import warnings

  import arviz as az
  import numpy as np
  import pandas as pd
  from matplotlib import pyplot as plt
  from matplotlib.axes._axes import Axes as MplAxes
#+end_src

* Loading the input data

Load a csv table ~m~ of prepared measurements.

#+begin_src jupyter-python :session py :exports both :results none :async yes :display plain
  DATA_DIR = os.path.join("data", "prepared", "tenfold")
  m = pd.read_csv(os.path.join(DATA_DIR, "input_df.csv"))
#+end_src

The table's rows report the mean reported value for each study/km combination
we retrieved from the BRENDA database, for the organisms Escherichia coli, Homo
Sapiens and Saccharomyces cerevisiae.

Here is what the first row looks like:

#+begin_src jupyter-python :session py :exports both :results raw drawer :async yes :display plain
  m.iloc[0]
#+end_src
#+RESULTS:
:results:
#+begin_example
  Unnamed: 0                                        0
  ec4                                       1.1.1.100
  organism                           Escherichia coli
  substrate                                     NADPH
  ec_sub                              1.1.1.100|NADPH
  org_sub                      Escherichia coli|NADPH
  literature                                 [704337]
  biology            1.1.1.100|Escherichia coli|NADPH
  y                                          -4.60517
  n                                                 1
  sd                                              NaN
  ec4_stan                                          1
  organism_stan                                     1
  substrate_stan                                    1
  ec_sub_stan                                       1
  org_sub_stan                                      1
  literature_stan                                   1
  biology_stan                                      1
  Name: 0, dtype: object
#+end_example
:end:

Here are the y values in order:

#+begin_src jupyter-python :session py :exports both :results drawer :async yes
  def plot_data(ax: MplAxes, m: pd.DataFrame, obs_col="y"):
      x = np.arange(len(m))
      y = m.sort_values(obs_col)[obs_col]
      ax.scatter(x, y, color="black", s=5, label="Observed")
      ax.get_xaxis().set_ticks([])
      ax.set(xlabel="Ec/substrate/organism in a study",
             ylabel="km ($\ln$ scale)")
      leg = ax.legend(frameon=False)
      return ax

  f, ax = plt.subplots()
  ax = plot_data(ax, m)
  f.savefig(
      os.path.join("results", "plots", "input_data.svg"),
      bbox_inches="tight"
  )
#+end_src

#+RESULTS:
:results:
:RESULTS:
#+attr_org: :width 394
[[file:./.ob-jupyter/7cf6715bf9152509a368ec51b6b0561c30df07f3.png]]
:END:
:end:

* Checking model results
Now we choose some files to analyse and load them into arviz [[https://arviz-devs.github.io/arviz/api/inference_data.html][InferenceData]]
objects.

#+begin_src jupyter-python :session py :exports both :results drawer :display plain :async yes
  RESULTS_DIR = os.path.join("results", "runs", "blk")
  idata = az.from_netcdf(os.path.join(RESULTS_DIR, "posterior", "idata.nc"))
  prior = az.from_netcdf(os.path.join(RESULTS_DIR, "prior", "idata.nc"))
  fake = az.from_netcdf(os.path.join(RESULTS_DIR, "fake", "idata.nc"))
  idata.add_groups({
     "prior": prior.posterior,
     "fake": fake.posterior,
     "observed_data_fake": fake.observed_data,
     "sample_stats_prior": prior.sample_stats
  })
  idata
#+end_src

#+RESULTS:
:results:
: /Users/tedgro/.pyenv/versions/3.8.6/lib/python3.8/site-packages/arviz/data/inference_data.py:1317: UserWarning: The group fake is not defined in the InferenceData scheme
:   warnings.warn(
: /Users/tedgro/.pyenv/versions/3.8.6/lib/python3.8/site-packages/arviz/data/inference_data.py:1317: UserWarning: The group observed_data_fake is not defined in the InferenceData scheme
:   warnings.warn(
: Inference data with groups:
: 	> posterior
: 	> log_likelihood
: 	> sample_stats
: 	> observed_data
: 	> prior
: 	> fake
: 	> observed_data_fake
: 	> sample_stats_prior
:end:


The next bit of code makes a pandas dataframe containing the marginal 2.5% and
97.5% predictive quantiles for each observation, for the prior, posterior and
fake-data-posterior models, as well as the relevant observed log km.


#+begin_src jupyter-python :session py :exports both :results drawer :display plain :async yes
  ppq_prior, ppq_posterior, ppq_fake = (
      idata.get(g)["yrep"]
      .quantile([0.025, 0.975], dim=["chain", "draw"])
      .to_series()
      .unstack("quantile")
      .add_prefix(f"{g}_")
      .rename_axis("", axis=1)
      for g in ("prior", "posterior", "fake")
  )
  ppq = ppq_prior.join(ppq_posterior).join(ppq_fake)
  ppq["obs"] = idata.observed_data["y_test"].values
  ppq["obs_fake"] = idata.observed_data_fake["y_test"].values
  ppq = ppq.sort_values("obs")
  ppq
#+end_src

#+RESULTS:
:results:
#+begin_example
              prior_0.025  prior_0.975  posterior_0.025  posterior_0.975  \
  yrep_dim_0                                                               
  1165          -7.847306     4.176406       -13.393017        -6.424276   
  135           -7.782696     3.902972        -6.582813        -0.101759   
  4031          -7.787102     3.821657       -16.086117         3.197087   
  4353          -8.103219     4.028205       -11.915835        -3.854086   
  3028          -7.791763     3.845696       -12.640320         1.228611   
  ...                 ...          ...              ...              ...   
  5262          -8.062909     4.032168        -6.012040         7.477105   
  5691          -8.200756     4.004418        -5.219667         1.685722   
  299           -7.903204     4.182943        -3.605605         4.699433   
  5648          -7.958700     4.143261         1.439340         8.232254   
  5143          -7.899296     3.928554        -4.157994         2.828056   

              fake_0.025  fake_0.975        obs  obs_fake  
  yrep_dim_0                                               
  1165         -6.279775   -1.592293 -17.777735 -4.224110  
  135          -5.991642   -1.630591 -16.436196 -4.141880  
  4031         -4.725760    0.647097 -15.855731  0.043783  
  4353         -2.135240    2.713712 -15.476242  0.265162  
  3028         -6.759330   -0.884686 -14.865333 -4.528010  
  ...                ...         ...        ...       ...  
  5262         -5.246686    0.309686   6.040255 -2.620180  
  5691         -3.634911    0.726355   6.492240 -1.913510  
  299          -5.198901   -0.589103   6.522093 -2.565750  
  5648         -6.388566   -1.837142   6.620919 -4.305300  
  5143         -5.626431   -0.802943   7.176002 -3.377470  

  [6389 rows x 8 columns]
#+end_example
:end:


This code plots these predictive distributions for the prior and posterior
models:

#+begin_src jupyter-python :session py :exports both :results raw drawer :async yes
  f, axes = plt.subplots(1, 2, sharey=True, figsize=[15, 5])
  axes = axes.ravel()
  d = ppq.sort_values("obs")
  x = np.arange(len(d))
  for g, ax in zip(["prior", "posterior"], axes):
      ax = plot_data(ax, d, obs_col="obs")
      ax.vlines(
          x, d[f"{g}_0.025"], d[f"{g}_0.975"],
          color="tab:blue", zorder=0, label="2.5%-97.5% interval"
      )
      leg = ax.legend(frameon=False)
      ax.set_title(g.capitalize())
  
  f.savefig(
      os.path.join("results", "plots", "ppc.svg"),
      bbox_inches="tight"
  )
#+end_src

#+RESULTS:
:results:
[[file:./.ob-jupyter/6cf630bb77131d709a5c9663c691ad1b4b09ecd4.png]]
:end:


It's interesting to compare these plots with the equivalent one for the fake data posterior:

#+begin_src jupyter-python :session py :exports both :results raw drawer :async yes
  f, ax = plt.subplots(figsize=[8, 5])
  d = ppq.sort_values("obs_fake")
  x = np.arange(len(d))
  ax = plot_data(ax, d, obs_col="obs_fake")
  ax.vlines(
      x, d["fake_0.025"], d["fake_0.975"],
      color="tab:blue", zorder=0, label="2.5%-97.5% interval"
  )
  leg = ax.legend(frameon=False)
  ax.set_title("Fake data posterior predictive distribution")
  f.savefig(
      os.path.join("results", "plots", "ppc_fake.svg"),
      bbox_inches="tight"
  )
#+end_src

#+RESULTS:
:results:
[[file:./.ob-jupyter/ba7539756502005f0d3d6bba94bc22d96a558a77.png]]
:end:


The posterior predictive distribution for the fake data model looks pretty similar to the real data one - great!

* Cross validation

We can try to evaluate the model's predictive performance using approximate
leave-one-out cross validation.

#+begin_src jupyter-python :session py :exports both :results raw drawer :async yes :display plain
  az.loo(idata, pointwise=True)
#+end_src

#+RESULTS:
:results:
: /Users/tedgro/.pyenv/versions/3.8.6/lib/python3.8/site-packages/arviz/stats/stats.py:655: UserWarning: Estimated shape parameter of Pareto distribution is greater than 0.7 for one or more samples. You should consider using a more robust model, this is because importance sampling is less likely to work well if the marginal posterior and LOO posterior are very different. This is more likely to happen with a non-robust model and highly influential observations.
:   warnings.warn(
#+begin_example
  Computed from 3200 by 6389 log-likelihood matrix

           Estimate       SE
  elpd_loo -12549.28    86.78
  p_loo     2681.82        -

  There has been a warning during the calculation. Please check the results.
  ------

  Pareto k diagnostic values:
                           Count   Pct.
  (-Inf, 0.5]   (good)     5914   92.6%
   (0.5, 0.7]   (ok)        407    6.4%
     (0.7, 1]   (bad)        65    1.0%
     (1, Inf)   (very bad)    3    0.0%
#+end_example
:end:

Unfortunately there are quite a few observations with high pareto k values, and arviz raises a warning.

To address this issue we also ran exact ten-fold cross validation.

The code below checks the results of these finds the posterior mean for
out-of-sample log likelihood for each split in our model and compares the
results with two alternative models:

#+begin_src jupyter-python :session py :exports both :results raw drawer :async yes :display plain
  run_dir = os.path.join("results", "runs")
  runs = [
      os.path.join(run_dir, s) for s in os.listdir(run_dir)
      if os.path.isdir(os.path.join(run_dir, s))
  ]
  for run in runs:
      run_ll = 0
      splits_dir = os.path.join(run, "splits")
      print(f"calculating out of sample log likelihoods for model {run}...")
      for split in os.listdir(splits_dir):
          idata = az.from_netcdf(os.path.join(splits_dir, split, "idata.nc"))
          split_ll = (
              idata.get("log_likelihood")["llik"]
              .mean(dim=["chain", "draw"])
              .values.sum()
          )
          print(f"\t{split}: {str(split_ll)}")
          run_ll += split_ll
      print(f"\ttotal out of sample log likelihood: {run_ll}")
#+end_src

#+RESULTS:
:results:
: calculating out of sample log likelihoods for model results/runs/blk...
: 	split_4: -1477.2729990849998
: 	split_3: -1481.00882687375
: 	split_2: -1441.2448831225
: 	split_5: -1477.1304594875
: 	split_0: -1467.7910366725
: 	split_7: -1480.53716923375
: 	split_9: -1504.9884546787503
: 	split_8: -1502.4060516825002
: 	split_6: -1483.4119273775
: 	split_1: -1515.77739092
: 	total out of sample log likelihood: -14831.569199133752
: calculating out of sample log likelihoods for model results/runs/simple...
: 	split_4: -2245.9115185025003
: 	split_3: -2182.5809449999997
: 	split_2: -2262.4983365774997
: 	split_5: -2150.721065105
: 	split_0: -2351.9525779625
: 	split_7: -2288.8911877974997
: 	split_9: -2194.1904108999997
: 	split_8: -2348.7044462475
: 	split_6: -2389.71007908
: 	split_1: -2263.04032354
: 	total out of sample log likelihood: -22678.2008907125
: calculating out of sample log likelihoods for model results/runs/really_simple...
: 	split_4: -1552.20558885
: 	split_3: -1505.90971685
: 	split_2: -1551.5482681499998
: 	split_5: -1470.82288385
: 	split_0: -1639.23052325
: 	split_7: -1557.061260825
: 	split_9: -1516.690492875
: 	split_8: -1651.7273156750002
: 	split_6: -1680.8369810999998
: 	split_1: -1575.714097825
: 	total out of sample log likelihood: -15701.747129249998
:end:
