#+TITLE: A statistical model summarising kinetic parameter information from the BRENDA database
#+BIBLIOGRAPHY: bibliography.bib
#+EXCLUDE_TAGS: noexport
#+STARTUP: overview
#+CITE_EXPORT: csl apa.csl
#+LATEX_COMPILER: latexmk

* Meta                                                             :noexport:
To regenerate the plots in this report, run the python script ~analyse.py~.

* Introduction

Our statistical model aims to summarise information from the BRENDA database
about Michaelis constants or "Km"s. We evaluated a range of modelling
approaches and chose one that balances coverage with good out-of-sample
predictive performance for the biological parameters that we judged would be
most interesting for systems biologists.

** Background

*** What is a michaelis constant

*** What is BRENDA

** State of the art

The problem of modelling BRENDA data about Michaelis constants has previously
been addressed in @borgerPredictionEnzymeKinetic2006 and
@krollDeepLearningAllows2021, among others.

@borgerPredictionEnzymeKinetic2006 models logarithmic-scale measurements
$y_{ijk}$ of a Km value for substrate $i$, ec number $j$ and organism $k$ using
the following ANOVA-style regression model:

\begin{equation}
y_ijk \sim N(\mu_i + \alpha_ij + \beta_ik, \sigma)
\end{equation}

In this expression the term $\mu$ represents substrate-specific effects,
whereas the terms $\alpha$ and $\beta$ respectively represent substrate-ec
number and substrate-organism interaction effects. $\sigma$ represents the
accuracy of the measurement apparatus.

The authors fit this measurement model to a subset of BRENDA data using maximum
likelihood estimation, obtain out of sample predictions using leave-one-out
cross validation and investigate patterns in the results.

This approach is limited by inability to incorporate prior information about
the plausible values of the main effects or their general trends. In addition,
the analysis produces point estimates: it does not attempt to fully capture the
available information about Kms.  This issue is particularly pertinent for
systems biologists who need to construct informative prior distributions for
kinetic models. For these users it is more important to know which possible Km
values are ruled out by the data in BRENDA than it is to know which make that
data most likely. Below we show that both of these issues with the approach in
@borgerPredictionEnzymeKinetic2006 can be addressed by incorporating a
hierarchical Baysian component.

@krollDeepLearningAllows2021 aims to predict Michaelis constants from BRENDA
and other sources by taking into account protein structure information using a
deep learning model. Exploiting this information allows for improved out of
sample predictive performance compared to models that do not use it.

The modelling approach in @krollDeepLearningAllows2021 does not avoid the
issues with lack of prior information and inaptness for downstream prior
modelling that we highlight above. In addition, the need for information about
protein structure limits the amount of Kms for which predictions can be
obtained. Again taking the point of view of a systems biologist attempting to
construct informative prior distributions, this is a severe problem as coverage
is at least as important a consideration for this application as precision.

Another problem with the current state of the art, which to our knowledge has
not yet been addressed, is that of modelling data from BRENDA about non-Km
kinetic parameters, such as Kcats.

[RESPECTFUL MODELLING FRAMEWORK]

* Method
In order to address the issues we highlight with previous work in this area, we
set out to model BRENDA data about Km and Kcat parameters using a hierarchical
Bayesian regression model. We fit this model to data that we fetched from
BRENDA using its publicly available API and evaluated its out of sample
predictive performance using approximate leave one out cross validation and
exact tenfold cross validation, comparing the results with some alternative
models.

We built and deployed an interactive webapp for inspecting our model's results
as well as providing an interface for downloading information that is relevant
for constructing informative prior distributions. We also provide source code
and data as well as an environment and instructions for exactly reproducing our
results.

** Statistical model 
*** Notational conventions
These models are described below using some notational conventions for
conciseness:

- $N(a, b)$ represents the normal distribution with mean $a$ and standard
  deviation $b$, i.e.

  $$
  \frac{1}{\sqrt{2 \pi}b} \exp\left(- \, \frac{1}{2}\left(  \frac{y-
  a}{b} \right)^2     \right)
  $$

- $ST(a, b, c)$ represents the student-T distribution with $a$ degrees of
  freedom, mean $b$ and standard deviation $c$, i.e.

  $$
  \frac{\Gamma\left((a +
  1)/2\right)}      {\Gamma(a/2)} \ \frac{1}{\sqrt{a \pi} \ c}
  \ \left( 1 + \frac{1}{a} \left(\frac{y - b}{c}\right)^2
  \right)^{-(a + 1)/2}  
  $$

- $gamma(a,b)$ represents the gamma distribution with parameters $a$ and $b$,
  i.e.

  $$
  \frac{b^{a}}{\Gamma(a)} \, y^{a - 1} \exp(-b \, y) 
  $$

- In these expressions the prefix $T$ indicates a truncated probability
  distribution, with the final two function arguments indicating the truncation
  points. For example, $TN(0, 1, 0, \infty)$ represents the standard half-normal
  distribution, i.e. the distribution $N(0,1)$, but with support only on the
  non-negative line.

- The symbol $\sim$ represents the relation of having a probability
  distribution: for example $a \sim N(0, 1)$ describes a model where the
  variable $a$ has a standard normal probability distribution.

- Subscripts represent indexes and superscripts represent labels. For example,
  the term $a^b_c$ denotes a variable $a$ with label $b$ that is indexed
  according to $c$. The reason for using superscript labels is to allow symbols
  like $\mu$, $\tau$ and $a$ to be re-used when the parameters they represent
  perform analogous functions.

*** Model specification

Our model consists of a measurement model specifying probability densities for
BRENDA data according to the values of some unknown parameters, and a prior
model specifying probability densities for those parameters so as to capture
information about them from other sources. We use essentially the same
measurement model as [cite:@borgerPredictionEnzymeKinetic2006], i.e.

\begin{equation}
y_{ijk} \sim ST(\nu, \mu + \alpha^{sub}_i + \alpha^{sub:ec}_{ij} + \alpha^{sub:org}_{ik}, \sigma)
\end{equation}

Our prior model includes hierarchical regression models for the parameters
$\alpha^{sub}$, $\alpha^{sub:ec}$ and $\alpha^{sub:org}$:

\begin{align*}
\alpha^{sub} &\sim N(0, \tau^{sub}) \\
\alpha^{sub:ec} &\sim N(0, \tau^{sub:ec}) \\
\alpha^{sub:org} &\sim N(0, \tau^{sub:org})
\end{align*}

To complete the model we also include semi-informative priors for the remaining
parameters:

\begin{align*}
\tau^{sub} &\sim TN(0, 1, 0, \infty) \\
\tau^{sub:ec} &\sim TN(0, 1, 0, \infty) \\
\tau^{sub:org} &\sim TN(0, 1, 0, \infty) \\
\mu &\sim N(-2, 1) \\
\nu &\sim T\Gamma(2, 0.1, 1, \infty) \\
\sigma &\sim TN(0, 2, 0, \infty) \\
\end{align*}

The priors for $\mu$ and $\sigma$ were chosen based on their quantiles - we
judged that it was very unlikely that the global mean log km would be less than
-5 or greater than 3 and the $HN(0,2)$ distribution similarly covers the range
of plausible standard deviations. We chose the prior for $\nu$ following the
analysis in [cite:@juarezModelBasedClusteringNonGaussian2010].

*** Comparison models

For comparison we tested a very simple model with just three parameters $\mu$,
$\nu$ and $\sigma$: respectively a global mean log km value and the degrees of
freedom and standard deviation of the student-t measurement error
distribution. The full model specification in our notation is as follows:

\begin{align*}
y_{ijk} &\sim ST(\nu, \mu, \sigma) \\
\nu &\sim gamma(2, 0.1) \\
\mu &\sim N(-1, 2) \\
\sigma &\sim TN(0, 2, 0, \infty)
\end{align*}

We also compared our model with a more complex alternative, adding a parameter
for each Km, as well as a partial pooling parameter $\tau^{km}$.

\begin{align*}
y_{ijk} &\sim ST(\nu, \mu + \alpha^{km}, \sigma) \\
\nu &\sim gamma(2, 0.1) \\
\mu &\sim N(-1, 2) \\
\alpha^{km} &\sim N(0, \tau^{km}) \\
\tau^{km} &\sim TN(0, 2, 0, \infty) \\
\sigma &\sim TN(0, 2, 0, \infty)
\end{align*}

The prior for $\tau^{km}$ was chosen based on our knowledge of the range of
plausible values for the variation of average log-scale km values.

** Data fetching and processing

We fetched data from BRENDA using the public SOAP API. See the script
~fetch_brenda_data.py~ and the directory ~data/raw/~ in the project github
repository for details. The raw data included three tables:

- output of the SOAP method ~getKmValue~, stored in ~data/raw/brenda_km_measurements.csv~
- output of the SOAP method ~getNaturalSubstrate~, stored in ~data/raw/brenda_natural_substrates.csv~
- output of the SOAP method ~getTemperatureOptimum~, stored in ~data/raw/brenda_temperature_optima.csv~

We made several significant data processing choices.

*** Preprocessing
The first data processing step was a non-destructive operations applied to all
reports. See the function ~preprocess~ in the module ~src/data_preparation.py~
for full details, and the file ~data/processed/km_preprocessed.csv~ for the
output. Briefly, we did the following:

- Edit column names so that they are lower case and broadly consistent with
  python naming conventions.
- Standardise null values such as ~-999~.
- Add natural ligands information (a ~frozenset~ valued column of the natural
  ligands for each report, if available, and a boolean column indicating if the
  target ligand is one of the natural ligands).
- Add real-valued ~temperature~, ~ph~ and ~mols~ columns by parsing the
  ~commentary~ field for each report. See the regular expressions
  ~NUMBER_REGEX~, ~TEMP_REGEX~, ~PH_REGEX~ and ~MOL_REGEX~ in the module
  ~src/data_preparation.py~ for details.
- Add substrate type column: this is either the name of the substrate if it is
  one of the manually specified cofactors listed in the variable ~COFACTORS~ in
  the module ~src/data_preparation.py~, or else "other"
- Add a ~biology~ column by concatenating the columns ~ec4~, ~organism~ and
  ~substrate~.

*** Filtering

We performed two successive filtering steps - one at the level of reports and
one at the level of biology/literature combinations. At the first step we
discared reports if they matched any of the following criteria:

- null values in the columns ~ec4~, ~km~, ~organism~ or ~substrate~
- negative or zero ~km~ value
- zero-valued ~ligand_structure_id~
- ~organism~ value not one those specified in the variable
  ~ORGANISMS_TO_INCLUDE~ in the module ~src/data_preparation.py~
- ~temperature~ value not between 5 and 50
- ~ph~ value not between 4 and 9
- ~is_natural~ column not ~True~

At the second stage of filtering, biology/literature combinations with fewer
than 2 observations were removed in order to prevent model bias due to sparsely
populated groups.

*** Grouping

Instead of modelling reports directly, we chose to group together reports with
the same biology and study, treating the mean log-scale km as a single
observation. We took this approach because of the presence in the BRENDA data
of different kinds of study. In some cases - presumably when the aim of a study
was to discover the sensitivity of a kinetic parameter to changes in
conditions - many reports with the same enzyme, organism, substrate and study
are available, with a range of different Km values and different experimental
conditions recorded in the ~commentary~ field. In other cases a study will
report only a single value for one kinetic parameter.

Due to this discrepancy it seemed wrong to treat reports from better populated
studies as equivalent to reports from more concise studies. While taking the
mean for a given study/biology combination before modelling destroys
information, we judged that it would lead to more realistic results than
treating each report as an observation, especially since we chose not to
attempt to model the effects of experimental conditions.

** Model evaluation procedure

We evaluated our models by fitting them to the data derived from the data
fetching and processing steps described above. We then estimated the models'
leave-one-out log predictive density using the Pareto-smoothed importance
sampling method described in [cite:@vehtariPracticalBayesianModel2017] and
implemented using the Python library Arviz [cite:@arviz_2019].

This procedure resulted in a number of warnings due to influential
observations. We therefore also performed also performed exact tenfold cross
validation, comparing the models' leave-10%-out log predictive density.

* Results

** Fit to fake data
To verify that our model would work under ideal model specification we fit it to
data that we generated using the model assumptions. To achieve this, we assigned
plausible values to the non-hierarchical parameters, then randomly generated
values for the hierarchical parameters, then used all the parameters to generate
random measurements, based on the real predictors. See the script
~generate_fake_data.py~ in the project repository for full details. The
resulting marginal posterior intervals are shown in [@fig:ppc_fake].

#+CAPTION: Marginal posterior predictive distributions for fake data fit
#+LABEL: ppc_fake
[[../results/plots/ppc_fake.svg]]

The cross validation results were as follows:

|--------------------------------------------+-----------------------------------------|
| Procedure                                  | average left-out log predictive density |
|--------------------------------------------+-----------------------------------------|
| approximate leave-one-out cross validation |                                         |
| exact tenfold cross validation             |                                         |
|--------------------------------------------+-----------------------------------------|


These results show that, if it precisely described the true data generating
process, our model would be capable of achieving an acceptable fit to our
dataset. While this is almost certainly not correct, the exercise is helpful
both for verifying the computation and for getting an idea of the upper limit of
the possible predictive performance given plausible parameter values.

** Fit to Real data

#+CAPTION: Marginal posterior predictive distributions for real data fits
#+LABEL: ppc_fake
[[../results/plots/ppc.svg]]

* Discussion
* References
#+PRINT_BIBLIOGRAPHY:


* Challenges                                                       :noexport:

Modelling The BRENDA dataset presents some specific challenges.

** EC numbers

BRENDA classifies enzymes according to a four level tree structure represented
by [[https://en.wikipedia.org/wiki/Enzyme_Commission_number][EC numbers]]. For example, the EC number 1.2.3.4 represents an oxidoreductase
(EC1 group 1) that acts on the aldehyde or oxo group of donors (EC2 group 1.2)
with oxygen as the acceptor (EC3 group 1.2.3) and specifically catalyses the
reaction $oxalate + O_2 + 2 H+ \rightleftharpoons 2 CO_2 + H_2O_2 $ (EC4 group 1.2.3.4).

We postulate that each component of an enzyme's EC number provide information
about the values of its kinetic parameters, and would like our statistical
model to be able to use this information. Unfortunately, doing so is not
straightforward. As the figure below shows, there are only fairly weak
systematic relationships between ec categories and average log km values.

[[./results/plots/log_km_means_by_ec.svg]]

However, the EC numbers also carry distributional information. The figure below shows the standard deviation of log km reports in each ec category. We can see that, for example, the standard deviation within EC1 category 2 tends to be somewhat lower than for EC1 category 1, and that there are also noticeable differences at the EC2 level.

[[./results/plots/log_km_sds_by_ec.svg]]

From this preliminary investigation it seems like, in order to take full advantage of the information provided by the EC number hierarchy, a distributional model will be required.

** Between-organism heterogeneity

It is not possible to ignore the information that BRENDA provides about
organisms, as the km value for an enzyme/substrate combination is often very
different for different organisms. To illustrate, the figure below shows a
histogram of differences between average measured km values for
enzyme/substrate combinations that were available for both /Homo sapiens/ and /Escherichia coli/. There are often differences of more than 3 on log scale, which is close to the average overall within-EC1 standard deviation.


[[./results/plots/raw_organism_differences.svg]]

** Experimental conditions

Not all the measurements are at standard conditions - for example some
measurements record unusual temperatures and pH values. Not all reports record these conditions, however. We therefore had to decide whether or not to attempt to model the effects of the available experimental conditions, and, if so, how to account for the cases where data about the experimental conditions is missing.

** Incomplete substrate information

BRENDA's API provides two fields from which the substrates for a given km
report can be inferred. One is a string called ~substrate~, which is a
human-readable name like 'ATP'. The other is an integer-valued id called
~ligandStructureId~. Unfortunately it is not possible to link these fields with
non-BRENDA identifiers, except through potentially unreliable string matching
operations on the ~substrate~ field. As a result, we chose not to include
detailed information about the substrates' chemical structures.

** Non-natural substrates

Many of the measurements in the BRENDA database relate to the km values of
enzyme/substrate combinations that do not occur naturally. These combinations
are less interesting for general systems biology applications, and might
introduce biases for the other substrates. On the other hand, it is also
possible that the unnatural substrates. It was therefore not clear in advance
whether or not it would be best to include the unnatural substrates in our
model.
